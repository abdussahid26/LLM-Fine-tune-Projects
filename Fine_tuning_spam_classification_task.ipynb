{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdussahid26/LLM-Fine-tune-Projects/blob/main/Fine_tuning_spam_classification_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN7bO79NDjBK"
      },
      "source": [
        "Downloading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp5t9UyW3yPz",
        "outputId": "431a07cf-0a65-479c-9bb9-c1f24ee008d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "import ssl\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "url=\"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    # Create an unverified SSL context\n",
        "    ssl_context = ssl._create_unverified_context()\n",
        "\n",
        "    # Downloading the file\n",
        "    with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "\n",
        "    # Add .tsv file extension\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path)\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glIzBBphDpdc"
      },
      "source": [
        "Loading the dataset into a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "uNOP3RjyDu8d",
        "outputId": "e962201f-b8e0-4f62-b8be-1f931cf0a675"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Label                                               Text\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...    ...                                                ...\n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568   ham               Will ü b going to esplanade fr home?\n",
              "5569   ham  Pity, * was in mood for that. So...any other s...\n",
              "5570   ham  The guy did some bitching but I acted like i'd...\n",
              "5571   ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2508d291-fba2-436d-a91b-f917790451a4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2508d291-fba2-436d-a91b-f917790451a4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2508d291-fba2-436d-a91b-f917790451a4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2508d291-fba2-436d-a91b-f917790451a4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ac4f59e7-ee39-4f37-83b1-0786d82d39d9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ac4f59e7-ee39-4f37-83b1-0786d82d39d9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ac4f59e7-ee39-4f37-83b1-0786d82d39d9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_1ddca369-cd2c-452c-b3b7-513624da1481\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_1ddca369-cd2c-452c-b3b7-513624da1481 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlIw36zoEEND"
      },
      "source": [
        "Let's examine the class label distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN6L0_LBELeD",
        "outputId": "8fb8061c-2e5a-475e-ec80-f7ace80a57b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df[\"Label\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkCqMyHIEWDw"
      },
      "source": [
        "For simplicity, and because we prefer a small dataset (which will facilitate faster fine-tuning of the LLM), we choose to undersample the dataset to include 747 instances from each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEvs4B-XElZr",
        "outputId": "388754ad-d402-41cd-807d-f74d6c9e6904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     747\n",
            "spam    747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def create_balanced_dataset(df):\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0] # Counts the instances of \"spam\"\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample( # Randomly samples \"ham\" instance to match the number of \"spam\" instances\n",
        "        num_spam, random_state=123\n",
        "    )\n",
        "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "    return balanced_df\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFrfOY3gGLj9"
      },
      "source": [
        "Next, we convert the \"string\" class labels \"ham\" and \"spam\" into integer class labels 0 and 1, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sSlDqKkMGUp7"
      },
      "outputs": [],
      "source": [
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO-4DiYEGjE4"
      },
      "source": [
        "This process is similar to converting text into token IDs. However, instead of using the GPT vocabular, which consists of 50,257 words, we are dealing with just two token IDs: 0 and 1.\n",
        "\n",
        "Next, we create a random_split function to split the dataset into three parts: 70% for training, 10% for validation, and 20% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C--dGBQIGyHd"
      },
      "outputs": [],
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "    df = df.sample(\n",
        "            frac=1, random_state=123\n",
        "        ).reset_index(drop=True) # Shuffle the entire DataFrame\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    # Splitting the DataFrame\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1) # Test size is simplified to be 0.2 as the remainder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlZ0j63EIVZ8"
      },
      "source": [
        "Let's save the dataset as CSV files so we can reuse it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B61U3n62IbZf"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okWutKSJItk5"
      },
      "source": [
        "Now we will set up the PyTorch data loaders that will be used to train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxsdJ5h_Kh6g"
      },
      "source": [
        "To batch these messages, we have two primary options:\n",
        "\n",
        "\n",
        "1.   Truncate all messages to the length of the shortest message in the dataset or batch.\n",
        "2.   Pad all messages to the length of the longest message in the dataset or batch.\n",
        "\n",
        "The first option is computationally cheaper, but it may result in significant information loss if shorter messages are much smaller than the average or longest messages, potentially reducing model performance. So, we opt for the second option, which preserves the entire content of all messages.\n",
        "\n",
        "To implement batching, where all messages are padded to the length of the longest message in the dataset, we add padding tokens to all shorter messages. For this purpose, we use **<|endoftext|>** as a padding token. However, instead of appending the string **<|endoftext|>** to each of the text messages directly, we can add the token ID corresponding to **<|endoftext|>** to the encoded text messages. 50256 is the token ID of the padding token **<|endoftext|>**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjP5fzo4Md5l",
        "outputId": "5d79e8f9-34e7-48a0-d1d5-66884c09c5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.2 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeeNgY1HI-lG",
        "outputId": "0f890621-ee08-4853-ca7c-a88350b41d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50256]\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud-UR8ceMuht"
      },
      "source": [
        "We first need to implement a PyTorch Dataset, which specifies how the data is loaded and processed before we can instantiate the data loaders. It also identifies the longest sequence in the training dataset, encodes the text messages, and ensures that all other sequences are padded with a padding token to match the length of the longest sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o425sh35M493"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
        "        ]\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "            # Truncates sequences if they are longer than max_length\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[:self.max_length]\n",
        "                for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "        # Pads sequences to the longest sequence\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text)) for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return(\n",
        "            torch.tensor(encoded, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            encoded_length = len(encoded_text)\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "        return max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-MjC70EbTcFc"
      },
      "outputs": [],
      "source": [
        "train_dataset = SpamDataset(\n",
        "    csv_file = \"train.csv\",\n",
        "    max_length = None,\n",
        "    tokenizer = tokenizer\n",
        ")\n",
        "\n",
        "val_dataset = SpamDataset(\n",
        "    csv_file = \"validation.csv\",\n",
        "    max_length = train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file = \"test.csv\",\n",
        "    max_length = train_dataset.max_length,\n",
        "    tokenizer = tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJmKCL6AUqhi",
        "outputId": "4ab5a375-f617-44f5-b191-0c58529b3f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n",
            "120\n",
            "120\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.max_length)\n",
        "print(val_dataset.max_length)\n",
        "print(test_dataset.max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLexWdWYVRG5"
      },
      "source": [
        "The code in the following listing creates the training, validation, and test set data loaders that load the text messages and labels in batch of size 8. For example, each batch will consist of eight training examples of length 120 and the corresponding class label of each example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UpZ4iKUBVpT-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0 # This setting ensures compatibility with most computers\n",
        "batch_size = 8\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = num_workers,\n",
        "    drop_last = True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset = val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = num_workers,\n",
        "    drop_last = False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = num_workers,\n",
        "    drop_last = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omVCC7XeWphN"
      },
      "source": [
        "To ensure that the data loaders are working and are, indeed, returning batches of the expected size, we iterate over the training loader and then print the tensor dimensions of the last batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcHUy9EaW-wm",
        "outputId": "6a19eb7b-7bb1-4d12-db40-7f6d17aa67cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch dimensions:  torch.Size([8, 120])\n",
            "Label batch dimensions:  torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "for input_batch, target_batch in train_loader:\n",
        "    pass\n",
        "\n",
        "print(\"Input batch dimensions: \", input_batch.shape)\n",
        "print(\"Label batch dimensions: \", target_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESeEh4XIXfS1"
      },
      "source": [
        "The input batches consist of eight training examples with 120 tokens each, as expected. The label tensor stores the class labels corresponding to the eight training examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpzi-mVGXuK_",
        "outputId": "3750167e-16eb-4d70-f63c-36075b7178ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130 training batches\n",
            "19 validation batches\n",
            "38 test batches\n"
          ]
        }
      ],
      "source": [
        "print(f\"{len(train_loader)} training batches\")\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "print(f\"{len(test_loader)} test batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4qZzlkLYHml"
      },
      "source": [
        "Now that we've prepared the data, we need to prepare the model for fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHRCw_VpzfIG"
      },
      "source": [
        "**Initializing a model with pretrained weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "p8Eevi9wYM8c"
      },
      "outputs": [],
      "source": [
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"],(\n",
        "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
        "    f\"length {BASE_CONFIG['context_length']}. Reinitialize datasets with \"\n",
        "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0po-ZoI16oV",
        "outputId": "af6945eb-1aa7-4349-d59f-3b5546b69543"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x7c071b3df790>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/abdussahid26/GPT-Model-from-Scratch-to-Generate-Text/main/gpt_download.py\"\n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "A3v-2u3252Py"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "\n",
        "        return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OOEJrNp63DgL"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ItJbLicF5mrr"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OIj1TzO85eHv"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # Shape: (batch, num_tokens, d_in)\n",
        "        queries = self.W_query(x)\n",
        "        keys = self.W_key(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a 'num_heads' dimension\n",
        "        # Unroll last dim: (batch, num_tokens, d_out) -> (batch, num_tokens, num_tokens, head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (batch, num_tokens, num_heads, head_dim) -> (batch, num_heads, num_tokens, head_dim)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3) # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (batch, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2dPJ8oqO5Ts0"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in = cfg[\"emb_dim\"],\n",
        "            d_out = cfg[\"emb_dim\"],\n",
        "            context_length = cfg[\"context_length\"],\n",
        "            num_heads = cfg[\"n_heads\"],\n",
        "            dropout = cfg[\"drop_rate\"],\n",
        "            qkv_bias = cfg[\"qkv_bias\"]\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x) # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut # Add the original input back\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VIBj_wBT4ILz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "\n",
        "        # The device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2JkhqXGl4Z6f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Pb-CpH9m4Z97"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params): # Sets the model's positional and token embedding weights to those specified in params.\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(gpt.trf_blocks[b].att.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(gpt.trf_blocks[b].att.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"] [\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(gpt.trf_blocks[b].ff.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(gpt.trf_blocks[b].ff.layers[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(gpt.trf_blocks[b].ff.layers[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(gpt.trf_blocks[b].norm1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(gpt.trf_blocks[b].norm1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(gpt.trf_blocks[b].norm2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(gpt.trf_blocks[b].norm2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lv5qvFD2XPy",
        "outputId": "e3e10597-652d-4489-cac0-fb3baf19edf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 54.9kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 5.63MiB/s]\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 125kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:07<00:00, 66.4MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 5.76MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 3.90MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 3.70MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-8H3aEch6lmH"
      },
      "outputs": [],
      "source": [
        "# idx is a (batch, n_tokens) array of indices in the current context\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g. if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only last 5 tokens are used as context.\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) # dimension of logits tensor: [batch, num_tokens, vocabulary_size]\n",
        "\n",
        "        # Focuses only on the last row, so that (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "        # Probs has shape (batch, vocab_size)\n",
        "        # Apply softmax to get probabilities\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        # idx_next has shape(batch, 1)\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "        # Append sample index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrM93JDZ58K2"
      },
      "source": [
        "After loading the model weights into the **GPTModel**, we reuse the text generation utility functions to ensure that the model generates coherent text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjxaKpak6Ng-",
        "outputId": "91dc0a8f-d8d1-4048-e679-f7c68121a86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every effort moves you forward.\n",
            "\n",
            "The first step is to understand the importance of your work\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "\n",
        "text_1 = \"Every effort moves you\"\n",
        "token_ids = generate_text_simple(\n",
        "    model = model,\n",
        "    idx = text_to_token_ids(text_1, tokenizer),\n",
        "    max_new_tokens = 15,\n",
        "    context_size = BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBPNokU57Pne"
      },
      "source": [
        "The model generated coherent text, which is indicates that the model weights have been loaded correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmrekMgE7wR1"
      },
      "source": [
        "Before we start fine-tuning the model as a spam classifier, let's see whether the model already classifies spam messages by prompting it with instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uukYSl9N7d0b",
        "outputId": "e6bf00ec-2aad-484e-cd8f-aa0efe124d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specifically selected to receive $1000 cash or a $2000 award.'\n",
            "\n",
            "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
          ]
        }
      ],
      "source": [
        "text_2 = (\n",
        "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
        "    \" 'You are a winner you have been specifically selected to receive $1000 cash or a $2000 award.'\"\n",
        ")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model = model,\n",
        "    idx = text_to_token_ids(text_2, tokenizer),\n",
        "    max_new_tokens = 23,\n",
        "    context_size = BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG3GKI8k8v2F"
      },
      "source": [
        "Based on the output, it's apparent that the model is **struggling** to follow instructions. This result is expected, as it has only undergone pretraining and lacks instruction fine-tuning. So let's prepare the model for classification fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxGUvUa69IeY"
      },
      "source": [
        "**Adding a Classification Head**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL-H1HaHCZHb"
      },
      "source": [
        "In this section, we will modify the pretrained LLM to prepare it for classification fine-tuning. To do so, we replace the original output layer, which maps the hidden representation to a vocabulary of 50,257, with a smaller output layer that maps to two classes: 0 (\"not spam\") and 1 (\"spam\"). We use the same model as before, except we replace the output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuJHL-3nDXuI"
      },
      "source": [
        "To get the model ready for classification fine-tuning, we first **freeze** the model, meaning that we make all layers nontrainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qCIR5DaIC7pc"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxM9b3j8Dul4"
      },
      "source": [
        "Let's replace the output layer (**model.out_head**), which originally maps the layer inputs to 50, 257 dimensions, the size of the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fM_fgG5nD-oG"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "num_classes = 2 # Spam and ham\n",
        "model.out_head = torch.nn.Linear(\n",
        "    in_features=BASE_CONFIG[\"emb_dim\"],\n",
        "    out_features=num_classes\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ziIAJQ2FinY"
      },
      "source": [
        "This new **model.out_head** output layer has its **require_grad** attribute set to **True** by default, which means that it's the only layer in the model that will be updated during training. Additionally, we configure the **last transformer block** and **the final LayerNorm** module, which connects this block to the output layer, to be trainable. To make the **final LayerNorm** and **last transformer block** trainable, we set their respective **require_grad** to **True**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Bg8xFJPkGWzP"
      },
      "outputs": [],
      "source": [
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdV7qgDBMt7Q"
      },
      "source": [
        "Even though we added a new output layer and marked certain layers as trainable or nontrainable, we can stil use this model similarly to how we have previously. For instance, we can feed it an example text identical to our previously used example text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-71roKhGE0f",
        "outputId": "d09c60df-1ea2-49b2-c58d-282b4dce7ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:  tensor([[5211,  345,  423,  640]])\n",
            "Inputs dimensions: torch.Size([1, 4])\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.encode(\"Do you have time\")\n",
        "inputs = torch.tensor(inputs).unsqueeze(0)\n",
        "\n",
        "print(\"Inputs: \", inputs)\n",
        "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9jVXNH6OygU"
      },
      "source": [
        "Then, we can pass the encoded token IDs to the model as usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3xRrcckO320",
        "outputId": "1caebbe7-6f1c-4d1a-90ae-859a2cac6b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs: \n",
            " tensor([[[-1.5854,  0.9904],\n",
            "         [-3.7235,  7.4548],\n",
            "         [-2.2661,  6.6049],\n",
            "         [-3.5983,  3.9902]]])\n",
            "Outputs dimensions:  torch.Size([1, 4, 2])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "\n",
        "print(\"Outputs: \\n\", outputs)\n",
        "print(\"Outputs dimensions: \", outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy40ktj6PiHy"
      },
      "source": [
        "Remember that we are interested in fine-tuning this model to return a class label indicating whether a model input is \"spam\" or \"not spam.\" We don't need to fine-tune all four output rows; instead, we can focus on a single output token. In particular, we will focus on the last row corresponding to the last output token. To extract the last output token from the output tensor, we use the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty7gPQTfQZdL",
        "outputId": "0b7cb6b9-7ef9-459e-cef8-a1ab2c452c7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last output token: tensor([[-3.5983,  3.9902]])\n"
          ]
        }
      ],
      "source": [
        "print(\"Last output token:\", outputs[:, -1, ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gZdHAVlRHRB"
      },
      "source": [
        "The last token in a sequence of causal attention mask accumulates the most information since it is the only token with access to data from all the previous tokens. Therefore, in our spam classification task, we focus on this last token during the fine-tuning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9nCrQnHVO-D"
      },
      "source": [
        "We can obtain the class label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTxDp9DCVR7L",
        "outputId": "89151aca-cdd5-4884-d2b6-59381a03354f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5.0598e-04, 9.9949e-01]])\n",
            "Class label:  1\n"
          ]
        }
      ],
      "source": [
        "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
        "label = torch.argmax(probas)\n",
        "\n",
        "print(probas)\n",
        "print(\"Class label: \", label.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq-N2AEsVrVA"
      },
      "source": [
        "In this case, the code returns 1, meaning the model predicts that the input text is \"spam.\" Using the softmax function here is optional because the largest outputs directly correspond to the highest probability scores. Hence, we can simplify the code without using softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbeHlP2cVK4m",
        "outputId": "d9856e80-ede4-46a3-c21c-2b0ce7c547e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class label:  1\n"
          ]
        }
      ],
      "source": [
        "logits = outputs[:, -1, :]\n",
        "label = torch.argmax(logits)\n",
        "\n",
        "print(\"Class label: \", label.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyx4MEvEXTuU"
      },
      "source": [
        "This concept can be used to compute the classification accuracy, which measures the percentage of correct predictions across a dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktbKAzaQRi4R"
      },
      "source": [
        "**Calculating the classification loss and accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvaioHzUTGbm"
      },
      "source": [
        "To determine the classification accuracy, we apply the argmax-based prediction code to all examples in the dataset and calculate the proportion of correct predictions by defining a calc-accuracy_loader function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "CgPt-6b-Y9OZ"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    if num_batches is not None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            input_batch = input_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_batch) [:, -1, :]  # Logits of last output token\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            num_examples += predicted_labels.shape[0]\n",
        "            correct_predictions += (\n",
        "                (predicted_labels == target_batch).sum().item()\n",
        "            )\n",
        "        else:\n",
        "            break\n",
        "    return correct_predictions / num_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96JuAmDtawGp"
      },
      "source": [
        "Let's use the function to determine the classification accuracies across various datasets estimated from 10 batches for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww3SwhJKatoT",
        "outputId": "bd43c63e-ab81-4d2a-a370-47154d1be469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 49.52%\n",
            "Validation accuracy: 53.02%\n",
            "Test accuracy: 50.33\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches = 10)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches = 10)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k309gExwcpwT"
      },
      "source": [
        "As we can see, the prediction accuracies are near a random prediction, which would be 50% in this case. To improve the prediction accuracies, we need to fine-tune the model.\n",
        "\n",
        "However, before we begin fine-tuning the model, we must define the loss function we will optimize during training. Our objective is to maximize the spam classification accuracy of the model, which means that the preceding code should output the correct class labels: 0 for non-spam and 1 for spam.\n",
        "\n",
        "Accordingly, the **calc_loss_batch** function remains the same, with one adjustment: we focus on optimizing only the last token, model(input_batch)[:, -1, :], rather than all tokens, model(input_batch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "2k3zLGGQd3w0"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch = input_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "\n",
        "    logits = model(input_batch)[:, -1, :] # logits of last output token\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZFrCpK2e0_h"
      },
      "source": [
        "We use the calc_loss_batch function to compute the loss for a single batch obtained from the previously defined data loaders. To calculate the loss for all batches in a dataloader, we define the calc_loss_loader function as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GShYBBRjfMG7"
      },
      "outputs": [],
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches = None):\n",
        "    total_loss = 0\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP5Lg3p5gIpa"
      },
      "source": [
        "Let's compute the initial loss for each dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htnbtlJJgMSz",
        "outputId": "644c016a-82b8-420a-8745-dc133cc8cc61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 2.4529\n",
            "Validation loss: 2.5830\n",
            "Test loss: 2.322\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad(): # Disables gradient tracking for efficiency because we are not training yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches = 5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches = 5)\n",
        "    test_loss = calc_loss_loader(test_loader, model, device, num_batches = 5)\n",
        "\n",
        "print(f\"Training loss: {train_loss:.4f}\")\n",
        "print(f\"Validation loss: {val_loss:.4f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8j9OEO1hl9D"
      },
      "source": [
        "Next, we will implement a training function to fine-tune the model, which means adjusting the model to minimize the training set loss. Minimize the training set loss will help increase the classification accuracy, which is our overall goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRkmbOxajC36"
      },
      "source": [
        "**Fine-tuning the model on supervised data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejpk7cFqsopO"
      },
      "source": [
        "In this section, we define and use the training function to fine-tune the pretrained LLM and improve its spam classification accuracy. The training loop is the same overall training loop we used earlier, with the only difference being that we calculate the classification accuracy instead of generating a sample text to evaluate the model.\n",
        "\n",
        "The training function also closely mirrors the train_model_simple function used for pretraining the model. The only two distinctions are that we now track the number of training examples seen (**examples_seen**) instead of the number of tokens, and we calculate the accuracy after each epoch instead of printing a sample text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1v5DF7eGt2X5"
      },
      "outputs": [],
      "source": [
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter):\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    for epoch in range(num_epochs): # Main training loop\n",
        "        model.train() # Sets model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Resets loss gradients from the previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculates loss gradients\n",
        "            optimizer.step() # Updates model weights using loss gradients\n",
        "            examples_seen += input_batch.shape[0] # New: tracks examples instead of tokens [8, 120]\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                print(f\"Ep {epoch + 1} (Step {global_step:06d}): \"\n",
        "                    f\"Train loss {train_loss:.3f}, \"\n",
        "                    f\"Val loss {val_loss:.3f}\"\n",
        "                )\n",
        "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "\n",
        "        print(f\"Training accuracy: {train_accuracy * 100: .2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy * 100: .2f}%\")\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "T55tdeZRz967"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH-Ej34T0mVO"
      },
      "source": [
        "Next, we initialize the optimizer, set the number of training epochs, and initiate the training using the train_classifier_simple function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzf_3jCM0xOC",
        "outputId": "f0ff3028-6dec-4be1-ff01-eefa1b9ebf1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n",
            "Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637\n",
            "Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557\n",
            "Training accuracy:  72.12% | Validation accuracy:  73.15%\n",
            "Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489\n",
            "Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397\n",
            "Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353\n",
            "Training accuracy:  84.62% | Validation accuracy:  85.23%\n",
            "Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320\n",
            "Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306\n",
            "Training accuracy:  87.21% | Validation accuracy:  91.28%\n",
            "Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200\n",
            "Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132\n",
            "Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137\n",
            "Training accuracy:  94.90% | Validation accuracy:  95.30%\n",
            "Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143\n",
            "Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074\n",
            "Training accuracy:  97.21% | Validation accuracy:  97.32%\n",
            "Training completed in 75.46 minutes.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "num_epochs = 5\n",
        "\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
        "    train_classifier_simple(\n",
        "        model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs, eval_freq=50, eval_iter=5\n",
        "    )\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHw3wfw114RQ"
      },
      "source": [
        "We then use Matplotlib to plot the loss function for the training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "zglA4LiLh7x3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "473e7c0d-19b1-4b04-c2b5-30b3a737c088"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "    ax1.plot(\n",
        "        epochs_seen, val_values, linestyle=\"-.\",\n",
        "        label=f\"Validation {label}\"\n",
        "    )\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(label.capitalize())\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2 = ax1.twiny()  # Creates a second x-axis for examples seen\n",
        "    ax2.plot(examples_seen, train_values, alpha=0) # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "    fig.tight_layout() # Adults layout to make room\n",
        "    plt.savefig(f\"{label}-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo4HRCcv4fq1"
      },
      "source": [
        "As we can see based on the sharp downward slope, the model is learning well from the training data, and there is little to no indiction of overfitting; that is, there is no noticeable gap between the training and validation set losses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JYqHbj7438r"
      },
      "source": [
        "Let's now plot the classification accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "D6FctwL34x_I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "a77c5a0a-b96c-4f2c-9261-d077bd38bc52"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msbDtC_u5poj"
      },
      "source": [
        "The model achieves a relatively high training and validation accuracy after epochs 4 and 5. Importantly, we previously set eval_iter=5 when using the train_classifier_simple function, which means our estimations of training and validation performance are based on only five batches for efficiency during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "2Lxwi05N6Gm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b308657-87f4-40b6-bb9d-8ad8d325c16f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 97.21%\n",
            "Validation accuracy: 97.32%\n",
            "Test accuracy: 95.67%\n"
          ]
        }
      ],
      "source": [
        "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=len(train_loader)) # Provide num_batches\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=len(val_loader)) # Provide num_batches\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=len(test_loader)) # Provide num_batches\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19I8A5a57WKm"
      },
      "source": [
        "The training and testset performance are almost identical. The slight discrepancy between the training and test set accuracies suggests minimal overfitting of the training data. Typically, the validation set accuracy is somewhat higher than the test set accuracy because the model development often involves tuning hyperparameters to perform well on the validation set, which might not generalize as effectively to the test set. This situation is common, but the gap could potentially be minimized by adjusting the model's settings, such as increasing the dropout rate (drop_rate) or the weight_decay parameter in the optimizer configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXGZv2dr8hwt"
      },
      "source": [
        "Having fine-tuned and evaluated the model, we are now ready to classify spam messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EtLwvPC7WcQ"
      },
      "source": [
        "**Using the LLM as a spam classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz6Sdd8-8q9E"
      },
      "source": [
        "Let's use our fine-tuned GPT-based spam classification model. The following following **classify_review** function follows data preprocessing steps similar to those we used in the **SpamDataset** implemented earlier. Then, after preprocessing, text into token IDs, the function uses the model to predict an integer class label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "wLlplF9a7aUf"
      },
      "outputs": [],
      "source": [
        "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
        "    model.eval()\n",
        "\n",
        "    input_ids = tokenizer.encode(text) # Prepares inputs to the model\n",
        "    supported_context_length = model.pos_emb.weight.shape[1]\n",
        "\n",
        "    input_ids = input_ids[:min(max_length, supported_context_length)] # Truncates sequences if they are too long\n",
        "\n",
        "    input_ids += [pad_token_id] * (max_length - len(input_ids)) # Pads sequences to the longest sequences\n",
        "\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # Adds batch dimension\n",
        "\n",
        "    with torch.no_grad(): # Models inference without gradient tracking\n",
        "        logits = model(input_tensor)[:, -1, :] # Logits of the last output token\n",
        "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    return \"spam\" if predicted_label == 1 else \"not spam\" # Returns the classified result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5Ivdu9Q--JG"
      },
      "source": [
        "Let's try this classify_review function on an example text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "tRp7Y-rj_Fa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a0a3b0c-a759-4866-b873-9267d8e4177e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n"
          ]
        }
      ],
      "source": [
        "text_1 = (\n",
        "    \"You are a winner you have been specifically selected to receive $1000 cash or a $2000 award.\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZDzLBaI_mud"
      },
      "source": [
        "The resulting model correctly predicts \"spam\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "yUg6-u3q_nmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362aff4b-be2a-4466-b22a-911416441281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not spam\n"
          ]
        }
      ],
      "source": [
        "text_2 = (\n",
        "    \"Hey, just wanted to check if we're still on for dinner tonight? Let me know!\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUBNH8DH_9xf"
      },
      "source": [
        "The model again makes a correct prediction and returns a \"not spam\" label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "802Zi4R___9I"
      },
      "source": [
        "Finally, let's save the model in case we want to reuse the model later without having to train it again. We can use the torch.save method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "A1tFLq0WAKoy"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"review_classifier.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wsi8JBBASZ4"
      },
      "source": [
        "Once saved, the model can be loaded by utilizing following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "DbjuQfGJAYUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51fdbf22-9819-4b8f-fe86-2e5771244505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-4d4235b68162>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_state_dict = torch.load(\"review_classifier.pth\", map_location=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "model_state_dict = torch.load(\"review_classifier.pth\", map_location=device)\n",
        "model.load_state_dict(model_state_dict)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPwA2HKNVP4aUGy/+uoFYuy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}